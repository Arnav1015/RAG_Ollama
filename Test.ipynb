{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb514f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "model_name = 'llama3.2'\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "]\n",
    "\n",
    "response = ollama.chat(model=model_name, messages=messages)\n",
    "print(\"Bot:\", response.message.content)\n",
    "\n",
    "while True:\n",
    "    \n",
    "    user_input = input(\"You: \")\n",
    "    if not user_input:\n",
    "        break  # exit loop on empty input\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    response = ollama.chat(model=model_name, messages=messages)\n",
    "    answer = response.message.content\n",
    "    print(\"Bot:\", answer)\n",
    "\n",
    "    messages.append({\"role\": \"assistant\", \"content\": answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54e6d55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Type': '/Page',\n",
       " '/MediaBox': [0, 0, 612, 792],\n",
       " '/Parent': {'/Type': '/Pages',\n",
       "  '/Kids': [IndirectObject(23, 0, 2946728418976),\n",
       "   IndirectObject(1, 0, 2946728418976)],\n",
       "  '/Count': 2},\n",
       " '/BleedBox': [0, 0, 612, 792],\n",
       " '/TrimBox': [0, 0, 612, 792],\n",
       " '/CropBox': [0, 0, 612, 792],\n",
       " '/ArtBox': [0, 0, 612, 792],\n",
       " '/Contents': [IndirectObject(37, 0, 2946728418976),\n",
       "  IndirectObject(42, 0, 2946728418976),\n",
       "  IndirectObject(44, 0, 2946728418976),\n",
       "  IndirectObject(46, 0, 2946728418976),\n",
       "  IndirectObject(48, 0, 2946728418976),\n",
       "  IndirectObject(56, 0, 2946728418976),\n",
       "  IndirectObject(58, 0, 2946728418976),\n",
       "  IndirectObject(114, 0, 2946728418976)],\n",
       " '/Resources': {'/Font': {'/C2_0': {'/Type': '/Font',\n",
       "    '/BaseFont': '/FNCIUM+Verdana-Bold',\n",
       "    '/Subtype': '/Type0',\n",
       "    '/Encoding': '/Identity-H',\n",
       "    '/DescendantFonts': [IndirectObject(82, 0, 2946728418976)],\n",
       "    '/ToUnicode': {'/Filter': '/FlateDecode'}},\n",
       "   '/C2_1': {'/Type': '/Font',\n",
       "    '/BaseFont': '/FNCIUM+Verdana',\n",
       "    '/Subtype': '/Type0',\n",
       "    '/Encoding': '/Identity-H',\n",
       "    '/DescendantFonts': [IndirectObject(83, 0, 2946728418976)],\n",
       "    '/ToUnicode': {'/Filter': '/FlateDecode'}},\n",
       "   '/C2_2': {'/Type': '/Font',\n",
       "    '/BaseFont': '/EFSCGO+Times-Roman',\n",
       "    '/Subtype': '/Type0',\n",
       "    '/Encoding': '/Identity-H',\n",
       "    '/DescendantFonts': [IndirectObject(66, 0, 2946728418976)],\n",
       "    '/ToUnicode': {'/Filter': '/FlateDecode'}},\n",
       "   '/C0_0': {'/Type': '/Font',\n",
       "    '/BaseFont': '/EFSCGO+Futura-ExtraBold',\n",
       "    '/Subtype': '/Type0',\n",
       "    '/Encoding': '/Identity-H',\n",
       "    '/DescendantFonts': [IndirectObject(84, 0, 2946728418976)],\n",
       "    '/ToUnicode': {'/Filter': '/FlateDecode'}},\n",
       "   '/C2_3': {'/Type': '/Font',\n",
       "    '/BaseFont': '/DXIWSQ+Verdana-Italic',\n",
       "    '/Subtype': '/Type0',\n",
       "    '/Encoding': '/Identity-H',\n",
       "    '/DescendantFonts': [IndirectObject(107, 0, 2946728418976)],\n",
       "    '/ToUnicode': {'/Filter': '/FlateDecode'}},\n",
       "   '/C2_4': {'/Type': '/Font',\n",
       "    '/BaseFont': '/DXIWSQ+Verdana-BoldItalic',\n",
       "    '/Subtype': '/Type0',\n",
       "    '/Encoding': '/Identity-H',\n",
       "    '/DescendantFonts': [IndirectObject(85, 0, 2946728418976)],\n",
       "    '/ToUnicode': {'/Filter': '/FlateDecode'}},\n",
       "   '/C2_5': {'/Type': '/Font',\n",
       "    '/BaseFont': '/CPYQES+TimesNewRomanPS-ItalicMT',\n",
       "    '/Subtype': '/Type0',\n",
       "    '/Encoding': '/Identity-H',\n",
       "    '/DescendantFonts': [IndirectObject(93, 0, 2946728418976)],\n",
       "    '/ToUnicode': {'/Filter': '/FlateDecode'}}},\n",
       "  '/ExtGState': {'/GS0': {'/Type': '/ExtGState',\n",
       "    '/OPM': 1,\n",
       "    '/SA': True,\n",
       "    '/op': True,\n",
       "    '/OP': True,\n",
       "    '/CA': 1,\n",
       "    '/ca': 1,\n",
       "    '/BM': '/Normal',\n",
       "    '/AIS': False,\n",
       "    '/SMask': '/None'},\n",
       "   '/GS1': {'/Type': '/ExtGState',\n",
       "    '/OPM': 0,\n",
       "    '/SA': True,\n",
       "    '/op': False,\n",
       "    '/OP': False,\n",
       "    '/CA': 1,\n",
       "    '/ca': 1,\n",
       "    '/BM': '/Normal',\n",
       "    '/AIS': False,\n",
       "    '/SMask': '/None'},\n",
       "   '/GS2': {'/Type': '/ExtGState',\n",
       "    '/OPM': 1,\n",
       "    '/SA': True,\n",
       "    '/op': False,\n",
       "    '/OP': False,\n",
       "    '/CA': 1,\n",
       "    '/ca': 1,\n",
       "    '/BM': '/Normal',\n",
       "    '/AIS': False,\n",
       "    '/SMask': '/None'}},\n",
       "  '/ProcSet': ['/PDF', '/Text']},\n",
       " '/Rotate': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pypdf  import PdfReader\n",
    "reader = PdfReader(\"sample.pdf\")\n",
    "page = reader.pages[0]\n",
    "page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2b24418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "reader = PdfReader(\"sample.pdf\")\n",
    "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    \n",
    "    for page in reader.pages:\n",
    "        t = page.extract_text()\n",
    "        if t:\n",
    "            f.write(t + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3298d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'Adobe PDF Library 5.0', 'creator': 'Adobe InDesign 2.0.2', 'creationdate': '2005-11-08T15:20:02+00:00', 'moddate': '2006-07-06T16:38:57-04:00', 'title': 'Sample Data for Data Tables', 'trapped': '/False', 'source': 'sample.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='Tutoring to Enhance Science Skills\\nTutoring Two: Learning to Make Data Tables..............................................................................................\\nSample Data for Data Tables\\n����������� �������� �������\\nNATIONAL PARTNERSHIP FOR QUALITY AFTERSCHOOL LEARNING\\nwww.sedl.org/afterschool/toolkits\\nUse these data to create data tables following the Guidelines for Making a Data Table and \\nChecklist for a Data Table.\\nExample 1: Pet Survey (GR 2–3)\\nMs. Hubert’s afterschool students took a survey of the 600 students at Morales Elementary \\nSchool. Students were asked to select their favorite pet from a list of eight animals. Here \\nare the results. \\nLizard 25, Dog 250, Cat 115, Bird 50, Guinea pig 30, Hamster 45, Fish 75, \\nFerret 10 \\nExample 2: Electromagnets—Increasing Coils (GR 3–5)\\nThe following data were collected using an electromagnet with a 1.5 volt battery, a switch, \\na piece of #20 insulated wire, and a nail. Three trials were run. Safety precautions in'), Document(metadata={'producer': 'Adobe PDF Library 5.0', 'creator': 'Adobe InDesign 2.0.2', 'creationdate': '2005-11-08T15:20:02+00:00', 'moddate': '2006-07-06T16:38:57-04:00', 'title': 'Sample Data for Data Tables', 'trapped': '/False', 'source': 'sample.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='repeating this experiment include using safety goggles or safety spectacles and avoiding \\nshort circuits.  \\n  Number of Coils         Number of Paperclips\\n 5 3, 5, 4\\n 10        7, 8, 6\\n 15  11, 10, 12\\n 20  15, 13, 14\\n    \\nExample 3: pH of Substances (GR 5–10)\\nThe following are pH values of common household substances taken by three different \\nteams using pH probes. Safety precautions in repeating this experiment include hooded \\nventilation, chemical-splash safety goggles, gloves, and apron. Do not use bleach, \\nammonia, or strong acids with children.\\nLemon juice 2.4, 2.0, 2.2; Baking soda (1 Tbsp) in Water (1 cup) 8.4, 8.3, 8.7; \\nOrange juice 3.5, 4.0, 3.4; Battery acid 1.0, 0.7, 0.5; Apples 3.0, 3.2, 3.5; \\nTomatoes 4.5, 4.2, 4.0; Bottled water 6.7, 7.0, 7.2; Milk of magnesia 10.5, 10.3, \\n10.6; Liquid hand soap 9.0, 10.0, 9.5; Vinegar 2.2, 2.9, 3.0; Household bleach \\n12.5, 12.5, 12.7; Milk 6.6, 6.5, 6.4; Household ammonia 11.5, 11.0, 11.5;'), Document(metadata={'producer': 'Adobe PDF Library 5.0', 'creator': 'Adobe InDesign 2.0.2', 'creationdate': '2005-11-08T15:20:02+00:00', 'moddate': '2006-07-06T16:38:57-04:00', 'title': 'Sample Data for Data Tables', 'trapped': '/False', 'source': 'sample.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='Lye 13.0, 13.5, 13.4; and Sodium hydroxide 14.0, 14.0, 13.9; Anti-freeze 10.1, \\n10.9, 9.7; Windex 9.9. 10.2, 9.5; Liquid detergent 10.5, 10.0, 10.3; and \\nCola 3.0, 2.5, 3.2\\nTeaching tip: The pH scale is from 0 to 14. Have students make two data tables, one \\nwith the data as given and one with the pH scale 0 to 14 with the substances’ average \\npH in rank order on the scale (Battery acid at the lower end and Sodium hydroxide at \\nthe upper end) or create a pH graphic organizer.\\n1'), Document(metadata={'producer': 'Adobe PDF Library 5.0', 'creator': 'Adobe InDesign 2.0.2', 'creationdate': '2005-11-08T15:20:02+00:00', 'moddate': '2006-07-06T16:38:57-04:00', 'title': 'Sample Data for Data Tables', 'trapped': '/False', 'source': 'sample.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}, page_content='© 2006 WGBH Educational Foundation. All rights reserved.\\nExample 4: Automobile Land Speed Records (GR 5-10)\\nIn the first recorded automobile race in 1898, Count Gaston de Chasseloup-Laubat of \\nParis, France, drove 1 kilometer in 57 seconds for an average speed of 39.2 miles per hour \\n(mph) or 63.1 kilometers per hour (kph). In 1904, Henry Ford drove his Ford Arrow across \\nfrozen Lake St. Clair, MI, at an average speed of 91.4 mph. Now, the North American \\nEagle is trying to break a land speed record of 800 mph. The Federation International de \\nL’Automobile (FIA), the world’s governing body for motor sport and land speed records, \\nrecorded the following land speed records. (Retrieved on February 5, 2006, from \\nhttp://www.landspeed.com/lsrinfo.asp.)\\nSpeed (mph)\\n407.447\\n413.199\\n434.22\\n468.719\\n526.277\\n536.712\\n555.127\\n576.553\\n600.601\\n622.407\\n633.468\\n763.035\\nDriver\\nCraig Breedlove\\nTom Green \\nArt Arfons\\nCraig Breedlove\\nCraig Breedlove\\nArt Arfons\\nCraig Breedlove\\nArt Arfons\\nCraig Breedlove'), Document(metadata={'producer': 'Adobe PDF Library 5.0', 'creator': 'Adobe InDesign 2.0.2', 'creationdate': '2005-11-08T15:20:02+00:00', 'moddate': '2006-07-06T16:38:57-04:00', 'title': 'Sample Data for Data Tables', 'trapped': '/False', 'source': 'sample.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}, page_content='Craig Breedlove\\nGary Gabelich\\nRichard Noble \\nAndy Green\\nCar\\nSpirit of America \\nWingfoot Express \\nGreen Monster \\nSpirit of America\\nSpirit of America\\nGreen Monster \\nSpirit of America, Sonic 1 \\nGreen Monster \\nSpirit of America, Sonic 1\\nBlue Flame \\nThrust 2 \\nThrust SSC\\nEngine\\nGE J47\\nWE J46  \\nGE J79 \\nGE J79 \\nGE J79 \\nGE J79  \\nGE J79 \\nGE J79 \\nGE J79 \\nRocket \\nRR RG 146 \\nRR Spey\\nDate\\n8/5/63\\n10/2/64\\n10/5/64\\n10/13/64\\n10/15/65\\n10/27/65\\n11/2/65 \\n11/7/65 \\n11/15/65 \\n10/23/70  \\n10/4/83  \\n10/15/97\\nExample 5: Distance and Time (GR 8-10)\\nThe following data were collected using a car with a water clock set to release a drop in \\na unit of time and a meter stick. The car rolled down an inclined plane. Three trials were \\nrun. Create a data table with an average distance column and an average velocity column, \\ncreate an average distance-time graph, and draw the best-fit line or curve. Estimate the \\ncar’s distance traveled and velocity at six drops of water. Describe the motion of the car. Is'), Document(metadata={'producer': 'Adobe PDF Library 5.0', 'creator': 'Adobe InDesign 2.0.2', 'creationdate': '2005-11-08T15:20:02+00:00', 'moddate': '2006-07-06T16:38:57-04:00', 'title': 'Sample Data for Data Tables', 'trapped': '/False', 'source': 'sample.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}, page_content='it going at a constant speed, accelerating, or decelerating? How do you know?\\n   Time (drops of water)           Distance (cm)\\n 1  10,11,9\\n 2  29, 31, 30\\n 3  59, 58, 61\\n 4  102, 100, 98\\n 5  122, 125, 127   \\n     \\n2')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"sample.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "#default chunk size and overlap is 1000 and 20 respectively\n",
    "#you can change these values as per your requirement\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 20\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
    "                                          chunk_overlap=chunk_overlap,\n",
    "                                          length_function=len,\n",
    "                                          separators=[\"\\n\\n\", \"\\n\", \" \"])\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "print (texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93ea6a36",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 127] The specified procedure could not be found. Error loading \"C:\\Users\\arnas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\lib\\c10_cuda.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 222\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m# Run the chat interface if executed as a script\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 222\u001b[0m     \u001b[43mollama_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 216\u001b[0m, in \u001b[0;36mollama_chat\u001b[1;34m()\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mollama_chat\u001b[39m():\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Main function to start the RAG-enabled Ollama chat\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m     rag \u001b[38;5;241m=\u001b[39m \u001b[43mOllamaRAG\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama3.2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m     rag\u001b[38;5;241m.\u001b[39mchat()\n",
      "Cell \u001b[1;32mIn[19], line 19\u001b[0m, in \u001b[0;36mOllamaRAG.__init__\u001b[1;34m(self, model_name, embedding_model, persist_directory)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m model_name\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Force CPU usage for embeddings to avoid CUDA issues\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpersist_directory \u001b[38;5;241m=\u001b[39m persist_directory\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Create vectorstore or load existing one\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_core\\_api\\deprecation.py:222\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     emit_warning()\n\u001b[1;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_community\\embeddings\\huggingface.py:84\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     warn_deprecated(\n\u001b[0;32m     75\u001b[0m         since\u001b[38;5;241m=\u001b[39msince,\n\u001b[0;32m     76\u001b[0m         removal\u001b[38;5;241m=\u001b[39mremoval,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m constructor instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     81\u001b[0m     )\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import sentence_transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sentence_transformers\\__init__.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     export_dynamic_quantized_onnx_model,\n\u001b[0;32m     11\u001b[0m     export_optimized_onnx_model,\n\u001b[0;32m     12\u001b[0m     export_static_quantized_openvino_model,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     15\u001b[0m     CrossEncoder,\n\u001b[0;32m     16\u001b[0m     CrossEncoderModelCardData,\n\u001b[0;32m     17\u001b[0m     CrossEncoderTrainer,\n\u001b[0;32m     18\u001b[0m     CrossEncoderTrainingArguments,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sentence_transformers\\backend.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Callable, Literal\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m disable_datasets_caching, is_datasets_available\n\u001b[0;32m     13\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sentence_transformers\\util.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hf_hub_download, snapshot_download\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor, device\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\__init__.py:270\u001b[0m\n\u001b[0;32m    266\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    268\u001b[0m         kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[1;32m--> 270\u001b[0m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_cuda_dep_paths\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;66;03m# Libraries can either be in path/nvidia/lib_folder/lib or path/lib_folder/lib\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\__init__.py:253\u001b[0m, in \u001b[0;36m_load_dll_libraries\u001b[1;34m()\u001b[0m\n\u001b[0;32m    249\u001b[0m     err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(last_error)\n\u001b[0;32m    250\u001b[0m     err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    252\u001b[0m     )\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    255\u001b[0m     is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] The specified procedure could not be found. Error loading \"C:\\Users\\arnas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\lib\\c10_cuda.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "class OllamaRAG:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name: str = \"llama3.2\",\n",
    "        embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "        persist_directory: str = \"./chroma_db\"\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Force CPU usage for embeddings to avoid CUDA issues\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=embedding_model,\n",
    "            model_kwargs={'device': 'cpu'}\n",
    "        )\n",
    "        \n",
    "        self.persist_directory = persist_directory\n",
    "        \n",
    "        # Create vectorstore or load existing one\n",
    "        if os.path.exists(persist_directory):\n",
    "            self.vectorstore = Chroma(\n",
    "                persist_directory=persist_directory,\n",
    "                embedding_function=self.embeddings\n",
    "            )\n",
    "        else:\n",
    "            self.vectorstore = Chroma(\n",
    "                embedding_function=self.embeddings,\n",
    "                persist_directory=persist_directory\n",
    "            )\n",
    "        \n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=20,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        \n",
    "        # Initialize messages for chat\n",
    "        self.messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Use the provided context to answer questions accurately. If the answer isn't in the context, say you don't know.\"},\n",
    "        ]\n",
    "    \n",
    "    def add_pdf(self, pdf_path: str, chunk_size: int = 1000, chunk_overlap: int = 20) -> None:\n",
    "        \"\"\"\n",
    "        Load a PDF document, split it into chunks, and add to the vector database.\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path to the PDF file\n",
    "            chunk_size: Size of text chunks (default: 1000)\n",
    "            chunk_overlap: Overlap between chunks (default: 20)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if file exists\n",
    "            if not os.path.exists(pdf_path):\n",
    "                print(f\"Error: PDF file not found at {pdf_path}\")\n",
    "                return\n",
    "                \n",
    "            # Load the PDF\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Configure text splitter\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=len,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \" \"]\n",
    "            )\n",
    "            \n",
    "            # Split documents into chunks\n",
    "            texts = text_splitter.split_documents(documents)\n",
    "            \n",
    "            # Add metadata to each chunk\n",
    "            for i, text in enumerate(texts):\n",
    "                if not text.metadata:\n",
    "                    text.metadata = {}\n",
    "                text.metadata[\"source\"] = pdf_path\n",
    "                text.metadata[\"chunk\"] = i\n",
    "            \n",
    "            # Add to vector database\n",
    "            self.vectorstore.add_documents(texts)\n",
    "            self.vectorstore.persist()\n",
    "            \n",
    "            print(f\"Successfully added PDF {pdf_path} to the vector database\")\n",
    "            print(f\"Added {len(texts)} chunks from the PDF\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing PDF {pdf_path}: {e}\")\n",
    "    \n",
    "    def add_text(self, text: str, metadata: Dict[str, Any] = None) -> None:\n",
    "        \"\"\"Add text to the vector database after splitting into chunks\"\"\"\n",
    "        chunks = self.text_splitter.split_text(text)\n",
    "        \n",
    "        # Add chunks to the vectorstore\n",
    "        metadatas = [metadata] * len(chunks) if metadata else None\n",
    "        self.vectorstore.add_texts(chunks, metadatas=metadatas)\n",
    "        self.vectorstore.persist()\n",
    "        print(f\"Added {len(chunks)} chunks to the vector database\")\n",
    "    \n",
    "    def add_file(self, file_path: str) -> None:\n",
    "        \"\"\"Add content from a file to the vector database\"\"\"\n",
    "        try:\n",
    "            # Check if file exists\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"Error: File not found at {file_path}\")\n",
    "                return\n",
    "                \n",
    "            # Check file extension to determine how to process it\n",
    "            file_extension = os.path.splitext(file_path)[1].lower()\n",
    "            \n",
    "            # Process PDF files\n",
    "            if file_extension == '.pdf':\n",
    "                self.add_pdf(file_path)\n",
    "            # Process text files\n",
    "            elif file_extension in ['.txt', '.md', '.csv', '.json']:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                metadata = {\"source\": file_path}\n",
    "                self.add_text(content, metadata)\n",
    "                print(f\"Successfully added {file_path} to the vector database\")\n",
    "            else:\n",
    "                print(f\"Unsupported file format: {file_extension}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding file {file_path}: {e}\")\n",
    "    \n",
    "    def query(self, query: str, k: int = 3) -> List[str]:\n",
    "        \"\"\"Retrieve relevant documents for a query\"\"\"\n",
    "        docs = self.vectorstore.similarity_search(query, k=k)\n",
    "        return docs\n",
    "    \n",
    "    def format_retrieved_context(self, docs: List) -> str:\n",
    "        \"\"\"Format retrieved documents into a context string\"\"\"\n",
    "        context = \"Here is relevant information:\\n\\n\"\n",
    "        \n",
    "        for i, doc in enumerate(docs):\n",
    "            context += f\"Document {i+1}:\\n{doc.page_content}\\n\"\n",
    "            if doc.metadata and \"source\" in doc.metadata:\n",
    "                context += f\"Source: {doc.metadata['source']}\\n\"\n",
    "            context += \"\\n\" + \"-\"*40 + \"\\n\"\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def chat(self):\n",
    "        \"\"\"Interactive chat interface with RAG capabilities\"\"\"\n",
    "        print(f\"Initialized Ollama RAG chatbot with model: {self.model_name}\")\n",
    "        print(\"Type 'exit' to quit\")\n",
    "        print(\"Type 'add file [path]' to add a text file\")\n",
    "        print(\"Type 'add pdf [path]' to add a PDF file\")\n",
    "        \n",
    "        # Initial greeting\n",
    "        response = ollama.chat(model=self.model_name, messages=self.messages)\n",
    "        print(\"Bot:\", response.message.content)\n",
    "        \n",
    "        while True:\n",
    "            user_input = input(\"You: \")\n",
    "            \n",
    "            if user_input.lower() == 'exit':\n",
    "                print(\"Exiting chat...\")\n",
    "                break\n",
    "                \n",
    "            # Command to add a text file to the knowledge base\n",
    "            if user_input.lower().startswith('add file '):\n",
    "                file_path = user_input[9:].strip()\n",
    "                self.add_file(file_path)\n",
    "                continue\n",
    "                \n",
    "            # Command to add a PDF file to the knowledge base\n",
    "            if user_input.lower().startswith('add pdf '):\n",
    "                pdf_path = user_input[8:].strip()\n",
    "                self.add_pdf(pdf_path)\n",
    "                continue\n",
    "            \n",
    "            # Retrieve relevant context from vector database\n",
    "            docs = self.query(user_input)\n",
    "            context = self.format_retrieved_context(docs)\n",
    "            \n",
    "            # Create RAG prompt with the retrieved context\n",
    "            rag_prompt = f\"\"\"\n",
    "Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer based on the context, say you don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {user_input}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "            \n",
    "            # Add the RAG prompt to messages\n",
    "            self.messages.append({\"role\": \"user\", \"content\": rag_prompt})\n",
    "            \n",
    "            # Get response from Ollama\n",
    "            response = ollama.chat(model=self.model_name, messages=self.messages)\n",
    "            answer = response.message.content\n",
    "            print(\"Bot:\", answer)\n",
    "            \n",
    "            # Update message history\n",
    "            # Remove the RAG prompt and replace with original question and answer\n",
    "            self.messages.pop()  # Remove the RAG prompt\n",
    "            self.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "            self.messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "\n",
    "def ollama_chat():\n",
    "    \"\"\"Main function to start the RAG-enabled Ollama chat\"\"\"\n",
    "    rag = OllamaRAG(model_name=\"llama3.2\")\n",
    "    rag.chat()\n",
    "\n",
    "\n",
    "# Run the chat interface if executed as a script\n",
    "if __name__ == \"__main__\":\n",
    "    ollama_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
